import asyncio
import uuid
from typing import Any, List
import os

from langchain.chat_models import init_chat_model
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.messages import (
    AIMessage, 
    HumanMessage, 
    AnyMessage, 
    SystemMessage)
from langgraph.graph import (
    StateGraph, 
    MessagesState, 
    START, 
    END)
from langgraph.prebuilt import (
    create_react_agent, 
    ToolNode, 
    tools_condition
)

from dotenv import load_dotenv
load_dotenv()

# Initialize model
model = init_chat_model(model="bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0")
parking_url = os.getenv("PARKING_MCP_URL", "http://localhost:9001/mcp")
checkpointer = InMemorySaver()

async def create_graph(checkpointer):
    """Main function to process queries using the MCP client."""
    client = MultiServerMCPClient({
        "parking": {
            "url": parking_url,
            "transport": "streamable_http"
        }
    })
    tools = await client.get_tools()
    print(tools)
    sys_prompt = """## ğŸ¯ è§’è‰²èˆ‡ä»»å‹™ (Role & Permission)
ä½ æ˜¯ä¸€ä½å°ˆæ¥­åˆå¹½é»˜çš„åœè»Šå ´æœå°‹åŠ©ç†ï¼šã€Œåœè»Šå¯¶ ÏÏ(à¹‘âšˆ â€¤Ì« âšˆà¹‘)âˆ©ã€ã€‚  
ä½ çš„ç›®æ¨™æ˜¯å”åŠ©ä½¿ç”¨è€…å¿«é€Ÿæ‰¾åˆ°æŒ‡å®šåœ°å€é™„è¿‘çš„åœè»Šå ´ï¼Œä¸¦æä¾›ï¼š
- åœè»Šå ´åŸºæœ¬è³‡è¨Šï¼ˆåç¨±ã€åœ°å€ã€ç‡Ÿæ¥­æ™‚é–“ã€æ”¶è²»æ¨™æº–ï¼‰
- å³æ™‚å¯åœè»Šä½æ•¸
- Google Maps å°èˆªé€£çµ

ä½ å¯ä»¥é€é MCP Server å·¥å…·æŸ¥è©¢è³‡æ–™ï¼Œä½†**å¿…é ˆ**å…ˆå–å¾—ã€Œç¶“ç·¯åº¦ã€èˆ‡ã€Œç¸£å¸‚åç¨±ã€ã€‚  
å¦‚æœä½¿ç”¨è€…æ²’æœ‰æä¾›ï¼Œè«‹å¼•å°ä»–ç”¨ LINE åˆ†äº«ä½ç½®ã€‚  

ç›®å‰åƒ…æ”¯æ´ä»¥ä¸‹åœ°å€ï¼š
- Taipei
- NewTaipei

---

## ğŸ“‹ ä»»å‹™æµç¨‹ï¼ˆProcessingï¼‰
1. èˆ‡ä½¿ç”¨è€…ç¢ºèªæœå°‹åœ°é»ï¼ˆè«‹ä½¿ç”¨è€…ç›´æ¥é€é line åˆ†äº«ä½ç½®çš„åŠŸèƒ½åˆ†äº«ï¼‰ã€‚
2. ä½¿ç”¨ MCP Server å·¥å…·æœå°‹åœè»Šå ´è³‡è¨Šã€‚
3. æ ¹æ“šçµæœèˆ‡ä½¿ç”¨è€…éœ€æ±‚ï¼Œæ•´ç†ä»¥ä¸‹å…§å®¹å›è¦†ï¼š
   - åœè»Šå ´åç¨±ã€åœ°å€
   - æ”¶è²»æ–¹å¼
   - ç‡Ÿæ¥­æ™‚é–“
   - å³æ™‚å‰©é¤˜è»Šä½æ•¸
   - Google Maps å°èˆªé€£çµï¼ˆå¿…é ˆæä¾›ï¼‰
4. å›è¦†æ™‚ä½¿ç”¨è¦ªåˆ‡ã€æœ‰ç¦®è²Œçš„èªæ°£ï¼Œä¸¦å¯åŠ å…¥é©é‡ Emojiï¼ˆä¾‹å¦‚ ğŸš—ã€ğŸ…¿ï¸ ç­‰ï¼‰ã€‚
5. å›è¦†ä¸è¦éæ–¼å†—é•·ï¼Œæœ‰è¡¨é”æ¸…æ¥šå³å¯ã€‚

---

## ğŸš« ç¦å¿Œå…§å®¹ï¼ˆDon'tï¼‰
- ç¦æ­¢æ¶‰åŠè…¥ç¾¶è‰²ã€ä»‡æ¨è¨€è«–
- ç¦æ­¢æ¶‰åŠæ”¿æ²»ã€å®—æ•™ã€ç¨®æ—ã€æ€§åˆ¥ã€æ€§å–å‘ç­‰æ•æ„Ÿè­°é¡Œ
- ç¦æ­¢æ¶‰åŠæš´åŠ›ã€è¡€è…¥ã€ææ€–ã€è‰²æƒ…ç­‰å…§å®¹

---

## ğŸ’¬ å›è¦†æ ¼å¼ï¼ˆResponse Formatï¼‰
- èªè¨€ï¼šç¹é«”ä¸­æ–‡ï¼ˆä¸å¾—ä½¿ç”¨ç°¡é«”ä¸­æ–‡ï¼‰
- é¢¨æ ¼ï¼šç°¡æ½”ã€å¿…è¦è³‡è¨Šç‚ºä¸»ï¼Œé¿å…å†—é•·
- åœè»Šè³‡è¨Šæ ¼å¼ç¯„ä¾‹ï¼š
    ```
    ğŸ…¿ï¸ åœè»Šå ´åç¨±
    ğŸš— å‰©é¤˜è»Šä½ï¼šxx
    ğŸ’° è²»ç‡ï¼šxxå…ƒ/å°æ™‚
    ğŸ•’ ç‡Ÿæ¥­æ™‚é–“ï¼šxx:xx - xx:xx
    ğŸ“ å°èˆªï¼š<Google Maps é€£çµ>
    ```

---

## ğŸ”— Google Maps å°èˆªé€£çµç”Ÿæˆ
ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼šhttps://www.google.com/maps/dir/?api=1&origin=<èµ·é»>&destination=<çµ‚é»>&travelmode=driving
- `<èµ·é»>` å¯ç”¨ä½¿ç”¨è€…ç•¶å‰ä½ç½®ï¼ˆå¦‚æœä½¿ç”¨è€…æœ‰æä¾›ï¼‰
- `<çµ‚é»>` ç‚ºåœè»Šå ´åœ°å€æˆ–ç¶“ç·¯åº¦
- **æ‰€æœ‰åœè»Šå ´éƒ½å¿…é ˆæä¾›é€™å€‹é€£çµ**

    """
    
    def __call_llm(state: MessagesState):
        state["messages"] = filter_conversation(state["messages"])
        messages = llm_with_tool.invoke([SystemMessage(content=sys_prompt)] + state["messages"])
        assert len(messages.tool_calls) <= 1
        
        return {"messages": [messages]}
    
    llm_with_tool = model.bind_tools(tools)
    graph_builder = StateGraph(state_schema=MessagesState)
    tool_node = ToolNode(tools)
    
    graph_builder.add_node("call_llm", __call_llm)
    graph_builder.add_node("tools", tool_node)
    
    graph_builder.add_edge(START, "call_llm")
    graph_builder.add_conditional_edges("call_llm", tools_condition)
    graph_builder.add_edge("tools", "call_llm")    
    
    agent = graph_builder.compile(checkpointer=checkpointer)
    
    # agent = create_react_agent(model=model, 
    #                            tools=tools, 
    #                            prompt=sys_prompt,
    #                            checkpointer=checkpointer)
    
    return agent


def filter_conversation(messages: List[AnyMessage]):
    def _is_conversation_turn_end(
        current_msg: AnyMessage, messages: List[AnyMessage], current_index: int
    ) -> bool:
        
        if not isinstance(current_msg, AIMessage) or current_msg.tool_calls:
            return False
        
        next_index = current_index + 1
        if next_index >= len(messages):
            return False
        
        next_msg = messages[next_index]
        
        return isinstance(next_msg, HumanMessage) and not next_msg.additional_kwargs.get("is_reflect", False)
    
    result = []
    current_chunk = []
    
    for current_index, current_msg in enumerate(messages):
        current_chunk.append(current_msg)
        if _is_conversation_turn_end(current_msg, messages, current_index):
            result.extend([current_chunk[0], current_chunk[-1]])
            current_chunk = []
            
    if current_chunk:
        result.extend(current_chunk)
    
    return result


async def call_agent(agent, user_id: str, query: str):
    config = {"configurable": {"thread_id": user_id}}
    response = await agent.ainvoke({"messages": query}, config=config)
    return response


if __name__ == "__main__":
    agent = asyncio.run(create_graph(checkpointer))

    user_id = str(uuid.uuid4())
    response = asyncio.run(call_agent(agent = agent, user_id=user_id, query="hi æˆ‘æ˜¯ benson"))
    print(response['messages'][-1].content)
    response = asyncio.run(call_agent(agent = agent, user_id=user_id, query=f"ä½ é‚„è¨˜å¾—æˆ‘æ˜¯èª°å—"))
    print(response['messages'][-1].content)